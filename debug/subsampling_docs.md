Number of docs used for generation of every graph:

-- Token OP bias:

all GPT-2: 1484
Pythia 6.9B: 1516 docs,
Pythia 12B: 1245 docs,
Pythia 2.8B deduped: 641 docs,
all others: 2813 docs

- without EOD:

Pythia 2.8B: 2233 docs,
Pythia 2.8B deduped: 2290 docs,
all others: 2814 docs

-Checkpoints:
For all: 264 docs used per checkpoint

-- Document OP bias:
OLMo 2 1B: 1612 docs,
7B: 10874 docs,
13B: 1966 docs

Tokens for each bin in pd_nll_t graph:
10235000 per bin
